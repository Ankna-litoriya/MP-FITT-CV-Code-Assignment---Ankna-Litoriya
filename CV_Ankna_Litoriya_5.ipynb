{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqqrVJTcB2fN",
        "outputId": "1d591eb8-e232-4e0b-b4fc-0a4ff3a2a43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted output: [0.48464867]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#define the sigmoid activation function,\n",
        "# which is commonly used in neural networks. It takes an input x and returns the sigmoid function's output.\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# define the derivative of the sigmoid function, which is used during backpropagation for calculating gradients.\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = sigmoid(self.output_layer_input)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, target, learning_rate):\n",
        "        loss = target - self.output\n",
        "        delta_output = loss * sigmoid_derivative(self.output)\n",
        "        loss_hidden = delta_output.dot(self.weights_hidden_output.T)\n",
        "        delta_hidden = loss_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(delta_output) * learning_rate\n",
        "        self.bias_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += self.inputs.T.dot(delta_hidden) * learning_rate\n",
        "        self.bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_size = 4\n",
        "    hidden_size = 8\n",
        "    output_size = 1\n",
        "\n",
        "    nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "    training_data = np.array([[0, 0, 1, 1], [0, 1, 1, 1], [1, 0, 1, 0], [1, 1, 1, 0]])\n",
        "    target_data = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    for _ in range(20000):\n",
        "        nn.forward(training_data)\n",
        "        nn.backward(target_data, learning_rate=0.001)\n",
        "\n",
        "    new_data = np.array([0, 1, 0, 0])\n",
        "    predicted_output = nn.forward(new_data)\n",
        "    print(f\"Predicted output: {predicted_output[0]}\")"
      ]
    }
  ]
}