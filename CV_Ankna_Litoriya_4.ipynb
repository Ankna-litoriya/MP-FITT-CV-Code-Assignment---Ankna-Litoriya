{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "img = image.load_img(r\"C:\\Users\\cheta\\OneDrive\\Desktop\\FIITOpenCV\")\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = np.expand_dims(x, axis=0)\n",
        "print(x)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "rotation_range=20,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1, save_to_dir='images/augdatabot',\n",
        "save_prefix='augmented_', save_format='jpg'):\n",
        "\n",
        "i += 1\n",
        "if i > 40:\n",
        "break\n",
        "\n",
        "# see folder structure\n",
        "import os\n",
        "for dirpath, dirnames, filenames in os.walk(\"drive/MyDrive/dataset\"):\n",
        "print(f\"There are {len(dirnames)} directories and {len(filenames)}\n",
        "images in '{dirpath}'.\")\n",
        "# View an image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "# Setup target directory (we'll view images from here)\n",
        "target_folder = target_dir+target_class\n",
        "\n",
        "# Get a random image path\n",
        "random_image = random.sample(os.listdir(target_folder), 1)\n",
        "\n",
        "# Read in the image and plot it using matplotlib\n",
        "\n",
        "img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "plt.imshow(img)\n",
        "plt.title(target_class)\n",
        "plt.axis(\"off\");\n",
        "\n",
        "print(f\"Image shape: {img.shape}\") # show the shape of the image\n",
        "\n",
        "return img\n",
        "# View a random image from the training dataset\n",
        "target_class = \"daisy/daisy\"\n",
        "# target_class = \"windflower/windflower\"\n",
        "img = view_random_image(target_dir=\"drive/MyDrive/dataset/train/\",\n",
        "\n",
        "target_class=target_class)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.regularizers import l2\n",
        "# Set the seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Preprocess data (get all of the pixel values between 1 and 0, also\n",
        "called scaling/normalization)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = \"drive/MyDrive/dataset/train/\"\n",
        "test_dir = \"drive/MyDrive/dataset/test/\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "\n",
        "batch_size=8, # number of\n",
        "\n",
        "images to process at a time\n",
        "\n",
        "target_size=(224, 224), #\n",
        "\n",
        "convert all images to be 224 x 224\n",
        "\n",
        "class_mode=\"binary\", # type\n",
        "\n",
        "of problem we're working on\n",
        "\n",
        "seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
        "batch_size=8,\n",
        "target_size=(224, 224),\n",
        "class_mode=\"binary\",\n",
        "seed=42)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "tf.keras.layers.Conv2D(filters=32,\n",
        "\n",
        "kernel_size=3, # can also be (3, 3)\n",
        "activation=\"relu\",\n",
        "input_shape=(224, 224, 3)), # first layer\n",
        "\n",
        "specifies input shape (height, width, colour channels)\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n",
        "padding=\"valid\"), # padding can also be 'same'\n",
        "\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.MaxPool2D(2),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation\n",
        "output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(),\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_data,\n",
        "epochs=10,\n",
        "steps_per_epoch=len(train_data),\n",
        "validation_data=valid_data,\n",
        "\n",
        "validation_steps=len(valid_data))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 7));\n",
        "\n",
        "# Apply Data augmentation\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "def apply_augmentation(class_name):\n",
        "input_folder = f\"drive/MyDrive/dataset/train/{class_name}\" # Folder\n",
        "containing original images\n",
        "output_folder = f\"drive/MyDrive/dataset/augmented/{class_name}\" #\n",
        "Folder to save augmented images\n",
        "\n",
        "# Define the path of the folder you want to create\n",
        "folder_path = 'drive/MyDrive/dataset/augmented'\n",
        "if not os.path.exists(folder_path):\n",
        "os.makedirs(folder_path)\n",
        "# Create an ImageDataGenerator instance for data augmentation\n",
        "os.makedirs(f\"drive/MyDrive/dataset/augmented/{class_name}\")\n",
        "datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "\n",
        "# Flow from directory with augmentation\n",
        "augmented_generator = datagen.flow_from_directory(\n",
        "input_folder,\n",
        "save_to_dir=output_folder,\n",
        "save_format='jpg')\n",
        "\n",
        "# Generate augmented images\n",
        "# You may need to adjust the batch size based on your dataset size\n",
        "for _ in range(10): # Generate 10 augmented images per original image\n",
        "augmented_generator.next()\n",
        "apply_augmentation(\"daisy\")\n",
        "apply_augmentation(\"windflower\")\n",
        "# Set the seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Preprocess data (get all of the pixel values between 1 and 0, also\n",
        "called scaling/normalization)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = \"drive/MyDrive/dataset/augmented/\"\n",
        "test_dir = \"drive/MyDrive/dataset/test/\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "\n",
        "batch_size=8, # number of\n",
        "\n",
        "images to process at a time\n",
        "\n",
        "target_size=(224, 224), #\n",
        "\n",
        "convert all images to be 224 x 224\n",
        "\n",
        "class_mode=\"binary\", # type\n",
        "\n",
        "of problem we're working on\n",
        "\n",
        "seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
        "batch_size=8,\n",
        "target_size=(224, 224),\n",
        "class_mode=\"binary\",\n",
        "seed=42)\n",
        "\n",
        "model_2 = tf.keras.models.Sequential([\n",
        "tf.keras.layers.Conv2D(filters=32,\n",
        "\n",
        "kernel_size=3, # can also be (3, 3)\n",
        "activation=\"relu\",\n",
        "\n",
        "input_shape=(224, 224, 3)), # first layer\n",
        "\n",
        "specifies input shape (height, width, colour channels)\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n",
        "padding=\"valid\"), # padding can also be 'same'\n",
        "\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "tf.keras.layers.MaxPool2D(2),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation\n",
        "output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss=\"binary_crossentropy\",\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(),\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_2 = model_2.fit(train_data,\n",
        "epochs=10,\n",
        "steps_per_epoch=len(train_data),\n",
        "validation_data=valid_data,\n",
        "validation_steps=len(valid_data))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history_2.history).plot(figsize=(10, 7));\n",
        "# Evaluate the model with augmentation\n",
        "loss_aug, acc_aug = aug_model.evaluate(test_data)\n",
        "print(\"Accuracy with augmentation:\", acc_aug)\n",
        "\n",
        "# Evaluate the model without augmentation\n",
        "loss_no_aug, acc_no_aug = model.evaluate(test_data)\n",
        "print(\"Accuracy without augmentation:\", acc_no_aug)"
      ],
      "metadata": {
        "id": "Wk_W1rRUBUt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}